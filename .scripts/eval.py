#!/usr/bin/env python3
# Run with python3 ./.scripts/run_gemini_from_file.py suggested-prompt-2025-06-29.md

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.cloud import aiplatform
import vertexai
#from vertexai.preview.evaluation import EvalTask
from vertexai.evaluation import EvalTask, PointwiseMetric

from google.cloud import logging
from datetime import datetime, timedelta
import os

# --- 1. Configuration (now using environment variables) ---
# --- These variables must be set in your Cloud Function environment ---
PROJECT_ID = os.environ.get("PROJECT_ID", "your-gcp-project-id")
LOCATION = os.environ.get("REGION", "us-central1")
BUCKET_NAME = os.environ.get("STAGING_GCS_BUCKET", "your-bucket-name")
# The LOG_NAME env var contains the short name of the log (e.g., 'agentspace_hcls_demo_log').
# The Cloud Logging filter requires the full resource name.
SHORT_LOG_NAME = os.environ.get("LOG_NAME", "run_gemini_from_file") # Default must match run_gemini_from_file.py
LOG_NAME = f"projects/{PROJECT_ID}/logs/{SHORT_LOG_NAME}"
JUDGEMENT_MODEL_NAME = os.environ.get("JUDGEMENT_MODEL_NAME", "gemini-2.5-flash")

# File to store the timestamp of the last run
TIMESTAMP_FILE = "last_run_timestamp.txt"

def get_last_run_timestamp():
    """Reads the timestamp of the last run from a local file."""
    try:
        with open(TIMESTAMP_FILE, "r") as f:
            return f.read().strip()
    except FileNotFoundError:
        return (datetime.utcnow() - timedelta(days=1)).isoformat() + "Z"

def save_current_timestamp():
    """Saves the current timestamp to a local file."""
    with open(TIMESTAMP_FILE, "w") as f:
        f.write(datetime.utcnow().isoformat() + "Z")

def get_logs_for_evaluation(last_run_timestamp: str) -> pd.DataFrame:
    """Queries Cloud Logging for all new logs since the last run."""
    logging_client = logging.Client(project=PROJECT_ID)
    
    # Add 'jsonPayload.request_id:*' to the filter to ensure we only process
    # the structured logs generated by the run_gemini_from_file.py script,
    # which always include a request_id. This prevents errors from trying
    # to process plain text logs that may also be in the stream.
    log_filter = (
        f'logName="{LOG_NAME}" AND '
        f'timestamp >= "{last_run_timestamp}" AND '
        f'jsonPayload.request_id:*'
    )
    
    log_entries = []
    print(f"Querying for structured logs in '{SHORT_LOG_NAME}' since: {last_run_timestamp}")

    for entry in logging_client.list_entries(filter_=log_filter):
        # The Cloud Logging API can return different entry types.
        # JsonEntry has its payload in `json_payload`.
        # StructEntry has its payload in `payload`.
        # We need to handle both to be robust.
        if hasattr(entry, 'json_payload'):
            payload = entry.json_payload
        elif hasattr(entry, 'payload') and isinstance(entry.payload, dict):
            payload = entry.payload
        else:
            continue # Skip entries that are not structured or don't have a payload.
        # We only need the prompt and response for the evaluation dataset.
        if all(k in payload for k in ['prompt', 'response']):
            log_entries.append({
                "prompt": payload['prompt'],
                "response": payload['response']
            })
    
    if not log_entries:
        print("No new structured log entries found for evaluation. Exiting.")
        return None

    return pd.DataFrame(log_entries)

def run_evaluation(event=None, context=None):
    """
    Main function to run the evaluation.
    """
    # This must be called first to set the project, location, and experiment context.
    aiplatform.init(project=PROJECT_ID, location=LOCATION, experiment=experiment_name)

    last_run = get_last_run_timestamp()
    
    eval_df = get_logs_for_evaluation(last_run)
    
    if eval_df is None or eval_df.empty:
        return

    print(f"Found {len(eval_df)} new log entries to evaluate.")

    experiment_name = "gemini-uncertainty-eval"
    run_name = f"eval-run-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

    # The start_run context manager can now be called because an experiment is set.
    with aiplatform.start_run(run=run_name):
        eval_task = EvalTask(
            dataset=eval_df,
            metrics=[
                "fluency",
                "coherence",
                "safety"
            ],
            judgement_model=JUDGEMENT_MODEL_NAME
        )

        evaluation_result = eval_task.evaluate()    

        summary_metrics = evaluation_result.summary_metrics

        labels = [key for key in summary_metrics.keys() if key.endswith('_score')]
        scores = [summary_metrics[key] for key in labels]

        num_vars = len(labels)
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        scores += scores[:1]
        angles += angles[:1]

        fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
        ax.plot(angles, scores, linewidth=2, linestyle='solid', label='Model Performance')
        ax.fill(angles, scores, 'b', alpha=0.1)
        ax.set_yticklabels([])
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels)
        ax.set_title(f'Evaluation of Gemini Run: {run_name}', size=12, color='black', va='center')
        ax.grid(True)
        
        image_path = f'gemini_evaluation_radar_chart_{run_name}.png'
        plt.savefig(image_path, bbox_inches='tight', dpi=150)
        plt.close(fig)

        # Use aiplatform.log_artifact, which requires an artifact_id.
        aiplatform.log_artifact(
            local_path=image_path,
            artifact_id="evaluation-radar-chart",
            display_name="Radar Chart",
        )
        print("Radar chart logged as a Vertex AI artifact.")

    save_current_timestamp()
    print("Script finished. The last run timestamp has been updated.")

if __name__ == "__main__":
    print("Running script from the command line.")
    run_evaluation()